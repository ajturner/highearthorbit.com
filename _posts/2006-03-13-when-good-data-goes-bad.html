---
layout: post
status: publish
published: true
title: When good data goes bad
author:
  display_name: Andrew
  login: admin
  email: andrew@highearthorbit.com
  url: http://highearthorbit.com
author_login: admin
author_email: andrew@highearthorbit.com
author_url: http://highearthorbit.com
wordpress_id: 377
wordpress_url: http://highearthorbit.com/when-good-data-goes-bad/
date: '2006-03-13 17:12:04 -0500'
date_gmt: '2006-03-13 22:12:04 -0500'
categories:
- Detroit
- Observation
- Web
tags: []
comments: []
---
<p><img src='http://highearthorbit.com/wp-images/ColdDay.JPG' alt='Very cold day' align='right' hspace='5px' vspace='5px' />Michigan has some very odd weather - snow one day, a balmy 70 deg F the next, then more snow. This is in April too, mind you. However, what I didn't expect, was <a href='http://www.wunderground.com/cgi-bin/findweather/getForecast?query=48067&hourly=1&yday=78&weekday=Monday'>next Monday's weather report</a>, a rather frigid <em>-9998 deg F</em>. Also note, that's the High temperature. When that is the high, I definitely imagine the low is <em>Not Applicable</em>, because at that point you probably don't care.</p>
<p>But this illuminates the point that you can't trust data, especially free data. There are some great Web API's (see <a href='http://www.programmableweb.com/'>Programmable Web</a> for a pretty comprehensive listing), which allow for mashup's, data sharing, and the ability to pull together great sets of data for all kinds of application. There have been a <a href='http://www.spatialdatalogic.com/cs/blogs/brian_flood/archive/2006/03/07/244.aspx'>variety</a> of <a href='http://blogs.zdnet.com/web2explorer/?p=128'>recent</a> <a href='http://batchgeocode.blogspot.com/2006/03/mashups-getting-mashed-by-data.html'>discussion</a> on who is responsible and holds the ultimate key to all this data and what this means to services that prop their business up on the availability, and most importantly, the <em>accuracy</em> of this data.</p>
<p>Imagine I have my house automation system to control the heating system based on the upcoming weather, or send warnings or other responses based on predicted weather patterns. A decent system may catch a complete outlier which is off any known physical chart. However, what if the weather was just off by 10 degrees, or given in the wrong units. When a human is in the loop, they may or may not catch this. But we also don't base large system responses on just one person saying "oh yeah, it'll be cold". However, when automated systems are fed data, we only have our foresight in designing the system to account for data integrity and accuracy and how best to evaluate these.</p>
<p>Perhaps one answer is providing multiple data sources, all of which help correlate, or catch errors in, other data sets. However, we can't be sure these data sets are truly independent of one another, and aren't just all <em>mashups</em> of the same data. This has been made very apparent in the rise of navigation systems and incorrect location data of business, streets, exits, etc. There are really only 2 major players in the world for navigational data: <a href='http://www.navteq.com/'>NavTeq</a> and <a href='http://www.teleatlas.com/'>TeleAtlas</a>. Therefore, Google, Yahoo!, MSN, MapQuest, Toyota/Lexus Nav systems, et al. are all just variable mashups on these same, limited, data sets (look closely at those credits on the bottom of the maps). Yet users will still "double-check" against various providers to "verify" an answer, even though the answers all came from the sam source. (which ultimately was a couple of workers riding around in cars talking into microphones and laptops, talking about what they see from their car)</p>
<p>So if you hear <a href='http://en.wikipedia.org/wiki/Hell,_Michigan'>Michigan actually freezes over</a> next week - then you can feel confident about your data. And just think, the <a href='http://www.epa.gov/sunwise/uvindex.html'>UV index</a> <em>isn't that bad</em>.</p>
